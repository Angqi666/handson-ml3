{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PMnzvx83CV_c"
   },
   "source": [
    "___\n",
    "\n",
    "<font size=+3 color=#FFBB00> HW: Projects and Training Models </font>\n",
    "\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "____\n",
    "\n",
    "<font size=+2 color=#00BBFF>Chapter 4 (30 points)</font>\n",
    "\n",
    "Read Chapter 4.  All of it. Very slowly and carefully. \n",
    "\n",
    "In three markdown cells, write your answers to questions (from the 3rd edition, p 174) 6, 10, and 11. Give very detailed explanations and use code if it helps. \n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### question 6\n",
    "> Is it a good idea to stop mini-batch gradient descent immediately when the validation error goes up?\n",
    "\n",
    "In the textbook, it's mentioned that in mini-batch gradient descent, the optimization process is subject to bouncing around. Halting training abruptly upon an increase in validation error might result in stopping prematurely, potentially before reaching the optimum.\n",
    "\n",
    "### question 10\n",
    "> Why would you want to use:\n",
    "\n",
    "> a. Ridge regression instead of plain linear regression(i.e.,without any regularization)?\n",
    "\n",
    "Normally, you want some control on the weight of each feature and ridge regression forces the learning algorithm to not only fit the data but also keep the model weights as small as possible, with l2 norm regularization term prevents the model from overfitting to the training data and improves its generalization performance on unseen data.\n",
    "\n",
    "> b. Lasso instead of ridge regression?\n",
    "          \n",
    "As stated in the textbook, Lasso regression utilizes an l1 norm regularization, which tends to set the weights of less important features to zero, thereby facilitating feature selection and creating a sparse model. However, this behavior is not guaranteed in all scenarios. Prior knowledge of the features is necessary to effectively achieve this outcome.\n",
    "          \n",
    "> c. Elastic net instead of lasso regression?\n",
    "\n",
    "Lasso often selects one predictor from a group of correlated predictors while setting the coefficients of others to zero, potentially causing instability and inconsistency in variable selection. Elastic Net resolves this by penalizing both large individual coefficients, similar to Lasso, and large groups of coefficients, akin to Ridge regression. This approach yields a more stable and robust selection of predictors, particularly beneficial when dealing with numerous correlated predictors in the dataset.\n",
    "\n",
    "\n",
    "### question 11\n",
    "\n",
    "> Suppose you want to classify pictures as outdoor/indoor and daytime/nighttime. Should you implement two logistic regression classifiers or one softmax regression classifier?\n",
    "\n",
    "I think train one logistic regression classifier to predict whether the picture is outdoor or indoor, and another logistic regression classifier to predict whether it's daytime or nighttime. Because these are not exclusive classes and all combinations are possible. Each classifier would output a probability, and the combination of these probabilities would represent the likelihood of each of the four combinations: outdoor daytime, outdoor nighttime, indoor daytime, and indoor nighttime.\n",
    "\n",
    "     \n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "_____\n",
    "\n",
    "<font size=+2 color=#00BBFF> Connecting Optimization to Statistics (25 points)</font>\n",
    "\n",
    "In a previous HW, you derived the linear regression (LR) parameters $w_0$ and $w_1$ in terms of sums over the data. Great! \n",
    "\n",
    "In this HW you will extend that idea to ensure you have some intuition for what those sums mean.  Derive the linear regression parameters $w_0$ and $w_1$ in terms of _statistical properties of the data_. In particular, in your final formulas use the precise definitions of mean and covariance:\n",
    "$$\\langle X\\rangle = \\frac{1}{N}\\sum_{i=1}^N X_i,$$\n",
    "$$\\mathrm{cov}(X, Y) = \\langle (X - \\langle X\\rangle)(Y - \\langle Y\\rangle),$$\n",
    "$$= \\mathbb{E}\\left[ (X - \\mathbb{E}[X])(Y - \\mathbb{E}[Y])\\right].$$\n",
    "\n",
    "Open a markdown cell and write your solution using [$\\LaTeX$](https://victoromondi1997.github.io/blog/latex/markdown/2020/07/03/Markdown-LaTeX.html). The steps might be:\n",
    "1. write down the loss function using MSE (and no regularization),\n",
    "2. the hypothesis/model is the line $w_0 + w_1x$,\n",
    "3. minimize the loss function,\n",
    "4. solve for $w_0$ and $w_1$,\n",
    "5. manipulate these expressions so that they are in terms of the statistical properties, mean and variance/covariance, of the data,\n",
    "6. overall, which statistical quantities are needed? are there statistical quantities that are not needed?  \n",
    "\n",
    "Finally, after your equations, comment on the ability of linear regression to find patterns in the data. Hint: in the lecture, I showed an animated plot, which included the [datasaurus](https://www.autodesk.com/research/publications/same-stats-different-graphs), which showed that many quite \"different\" datasets have the same low-order statistics.\n",
    "\n",
    "_Hint:_ Don't make this problem too hard - it is rather easy. In the previous HW, you already did the hard part. Here what you need to do is simply recognize statistical definitions in the sums you got. The only non-trivial step might be to add and subtract terms, like a mean, to \"find\" a variance. (For example, $\\langle X\\rangle = \\langle X + \\langle X\\rangle - \\langle X\\rangle $.) Think of it like solving a puzzle: you move around, and add and substract, terms until all of the sums can be interpreted as a known statistical quantity. In the end, you will have connected:\n",
    "* loss function\n",
    "* solution to an optimization problem\n",
    "* the results in terms of meaningful statistical quantities. \n",
    "\n",
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The loss function and with line $w_0 + w_1x$:\n",
    "\n",
    "$$ \\mathcal{L} (w_0,w_1) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - (w_1x_i+w_0))^2 $$\n",
    "\n",
    "2. To minimize the loss function, we differentiate it with respect to $w_0$ and $w_1$ and set the derivatives to zero.\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial w_1} = -\\frac{2}{n} \\sum_{i=1}^{n} x_i(y_i - (w_1x_i+w_0)) = 0\\tag{1}$$    \n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial w_0} = -\\frac{2}{n} \\sum_{i=1}^{n} (y_i - (w_1x_i+w_0)) = 0\\tag{2}$$     \n",
    "\n",
    "\n",
    "\n",
    "First, look at eqn. (2) is much simpler than eqn. (1) so I skipped the most of the part and come to the statistical form of $w_0$:\n",
    "$$\n",
    "w_0 = \\langle y \\rangle - w_1 \\langle x \\rangle\n",
    "$$\n",
    "\n",
    "Then, look at eqn. (1) by expanding and rearranging terms:\n",
    "$$\n",
    "w_1 = \\frac{\\sum_{i=1}^{N} x_i y_i - w_0 \\sum_{i=1}^{N} x_i}{\\sum_{i=1}^{N} x_i^2}\n",
    "$$\n",
    "Since $w_0 = \\langle y \\rangle - w_1\\langle x \\rangle$, so $w_1$ can be written as:\n",
    "$$\n",
    "w_1 = \\frac{\\sum_{i=1}^{N} x_i y_i - (\\langle y \\rangle - w_1\\langle x \\rangle) \\sum_{i=1}^{N} x_i}{\\sum_{i=1}^{N} x_i^2}\n",
    "$$\n",
    "Now, rearranging terms to isolate $w_1$:\n",
    "$$\n",
    "w_1 - w_1\\langle x \\rangle\\frac{\\sum_{i=1}^{N} x_i}{\\sum_{i=1}^{N} x_i^2} = \\frac{\\sum_{i=1}^{N} x_i y_i - \\langle y \\rangle  \\sum_{i=1}^{N} x_i}{\\sum_{i=1}^{N} x_i^2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "w_1  = \\frac{\\sum_{i=1}^{N} x_i y_i - \\langle y \\rangle  \\sum_{i=1}^{N} x_i}{\\sum_{i=1}^{N} x_i^2 -\\langle x \\rangle\\sum_{i=1}^{N} x_i}\n",
    "$$\n",
    "\n",
    "By multiply $\\frac{1}{N}$:\n",
    "\n",
    "$$\n",
    "w_1  = \\frac{\\frac{1}{N}\\sum_{i=1}^{N} x_i y_i - \\langle y \\rangle  \\frac{1}{N}\\sum_{i=1}^{N} x_i}{\\frac{1}{N}\\sum_{i=1}^{N} x_i^2 -\\langle x \\rangle\\frac{1}{N}\\sum_{i=1}^{N} x_i} \\tag{3}\n",
    "$$\n",
    "\n",
    "Rewrite the eqn. (3) using statistical term:\n",
    "\n",
    "$$\n",
    "w_1  = \\frac{\\mathbb{E}[xy]- \\mathbb{E}[x]\\mathbb{E}[y]}{\\mathbb{E}[x^2] -(\\mathbb{E}[x])^2} \n",
    "$$\n",
    "\n",
    "In terms of the statistical properties, mean and variance/covariance, $w_1$ can rewrite into:\n",
    "$$\n",
    "w_1 = \\frac{\\text{cov}(x, y)}{\\text{var}(x)}\n",
    "$$\n",
    "\n",
    "**Understanding**: \n",
    "\n",
    "Covariance measures the degree to which two variables (in this case, x and y) change together. A positive covariance indicates that as one variable increases, the other tends to increase as well, while a negative covariance indicates an inverse relationship. In the context of linear regression $\\text{cov}(x, y)$ represents the extent to which the independent variable x and the dependent variable y vary together. A higher covariance implies that x and y are more strongly related. It seems to me, that covariance of x and y give a relationship between x and y (positive, negative or non), that will give the sign of the weight $w_1$. And also if the correlation big then $w_1$ will be large as well.\n",
    "\n",
    "Variance of x: Variance measures the spread or dispersion of a set of data points around their mean. A larger variance indicates that the data points are more spread out from the mean. If the variance is small that mean x is more concentrate and is can be a good feature so that the weight $w_1$ will be increase.\n",
    "\n",
    "In the context of linear regression, $w_0$ can be interpreted as a bias term. in linear regression serves to ensure that the model captures not only the relationship between the independent and dependent variables but also any constant offset or bias in the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CUQc3-CpZb3E"
   },
   "source": [
    "\n",
    "_____\n",
    "\n",
    "<font size=+2 color=#00BBFF> Projects (45 points) </font>\n",
    "\n",
    "We have been slowly developing your projects, mostly with the idea that you think through most of the details early so that you have time to pivot if needed. \n",
    "\n",
    "Importantly, don't worry or get stressed about any of this at this point. You won't need to really lock down your project until after spring break. What we are doing now is getting you to think about what will work and what won't. \n",
    "\n",
    "In fact, it is also possible that your project could have problems at the very end of the semester. Although we are doing everything in our power to make that not happen, it does happen. There is some risk in allowing you to set up your own project and nearly any topic. But, we also have ways to ensure nothing bad can happen in the end. Have fun, play, don't stress. \n",
    "\n",
    "Importantly, you need to be able to pivot when things don't look promising. \n",
    "\n",
    "This week you will read three preproposals written by other students for these two reasons:\n",
    "* to give them detailed feedback on their project (as it currently exists),\n",
    "* to give you a critical eye into thinking about ML projects.\n",
    "\n",
    "You have been assigned three proposals. You will write a **report on each of them** that will be given to its author. You are going to **submit a separate report as pdf for each preproposal review**. Similarly, you will get three reviews of your own proposal! \n",
    "\n",
    "## 3 reviews for 3 preproposal, will be submitted to D2L, name of each report is the last name of preproposal author you are reviewing\n",
    "\n",
    "Each of your reviews will be at least one page (use more if needed), 11pt font, 1\" margins, with the name of the student and the project at the top. \n",
    "\n",
    "Do not include your name at any part of the review document. It is customary in the sciences to keep reviewers anonymous. This allows the reviewers to be completely honest. It also allows the reviewers to be jerks - let me know if you see any that. The goal here is to be as helpful as possible to your classmates. Of course, if you want to reveal who you are because you want to have a longer conversation with someone (maybe your projects are similar), feel free to do that. \n",
    "\n",
    "There are two main areas of focus at this stage:\n",
    "1. **Does the proposal reveal to you a specific goal? If it does, summarize that goal. In your summary, be detailed about the goal. Is the goal something within ML itself? Or is the goal related to some specific outcome? In what application area is this goal? What field? Did they sell you on it? Are you convinced? What part of ML? This discussion should be about $1/3$ of a page.** (think about the project in the following manner: as part of each homework and in-class assignment you have and you will implement various ML, NLP, or even RL models on different datasets and for different purposes. Is the project only going to implement ML models on some datas and that implementation is the goal? (based on the syllabus, the homework and the project has the same weight, so what's included in the project must not be the same as what has been done weekly for each homework, i.e., purely utilizing ML on a dataset))\n",
    "2. Read Appendix A, p. 779, of your textbook. This is the basic structure for a project that we will be following. (There will be some exceptions for unusual projects.) For each of the headings (**Frame the Problem**, **Get the Data**, **Explore the Data**, etc.), see if the author addressed the items listed. If they did and they did a good job, tell them why. If there is something very much missing from the proposal, like IDA/EDA, be sure to mark that as an area to focus on in the future. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FsaxlLuAflGm"
   },
   "source": [
    "### You can find the preproposal files in [this link](https://drive.google.com/drive/folders/15qo-EjVi3Xxim7D1n6jqJ3d7QOxKRPk0?usp=drive_link)\n",
    "\n",
    "### Preproposal assignment reviews\n",
    "\n",
    "| Name | Review 1 | Review 2 | Review 3 |\n",
    "|---|---|---|---|\n",
    "| Angqi Li | Spencer Lee | Max Gregg | Arsh Ahtsham  |\n",
    "| Arman Khoshnevis | Chris Gerlach | Shimeng Dai | Trent Henry |\n",
    "| Arsh Ahtsham  | Ashkan Bagherzadeh Khodashahri | Austin Rodriguez Guardia  | Shubham Koirala  |\n",
    "| Ashkan Bagherzadeh Khodashahri | Austin Rodriguez Guardia  | Shubham Koirala  | Eli Broemer |\n",
    "| Austin Rodriguez Guardia  | Shubham Koirala  | Eli Broemer | Md Faisal |\n",
    "| Bingqing Wang | Sardar Nafis Bin Ali | Zhiyuan Tian | Steven Strachan  |\n",
    "| Chris Gerlach | Shimeng Dai | Trent Henry | Fawaz Imtiaz |\n",
    "| Danny Jammooa | Mikayla Norton | Hakan Burak Karli | Arman Khoshnevis |\n",
    "| Eli Broemer | Md Faisal | Jeeva Saravana Bhavanandam | Yan Lyu |\n",
    "| Faith Houck  | Lan Jin | Yash Vikas Mandlecha | Maruf Md Ikram |\n",
    "| Fawaz Imtiaz | Angqi Li | Spencer Lee | Max Gregg |\n",
    "| Hakan Burak Karli | Arman Khoshnevis | Chris Gerlach | Shimeng Dai |\n",
    "| Jeeva Saravana Bhavanandam | Yan Lyu | Neel Joshi  | Yuhan Zhu |\n",
    "| Josiah Hill  | Kang Ho Lee  | Prakash K C | William Fung |\n",
    "| Kang Ho Lee  | Prakash K C | William Fung | Danny Jammooa |\n",
    "| Lan Jin | Yash Vikas Mandlecha | Maruf Md Ikram | Zhongren Xu |\n",
    "| Liantao Li  | Faith Houck  | Lan Jin | Yash Vikas Mandlecha |\n",
    "| Maruf Md Ikram | Zhongren Xu | Josiah Hill  | Kang Ho Lee  |\n",
    "| Max Chumley | Liantao Li  | Faith Houck  | Lan Jin |\n",
    "| Max Gregg | Arsh Ahtsham  | Ashkan Bagherzadeh Khodashahri | Austin Rodriguez Guardia  |\n",
    "| Md Faisal | Jeeva Saravana Bhavanandam | Yan Lyu | Neel Joshi  |\n",
    "| Mikayla Norton | Hakan Burak Karli | Arman Khoshnevis | Chris Gerlach |\n",
    "| Neel Joshi  | Yuhan Zhu | Patrick Cook | Patrick Govan |\n",
    "| Patrick Cook | Patrick Govan | Bingqing Wang | Sardar Nafis Bin Ali |\n",
    "| Patrick Govan | Bingqing Wang | Sardar Nafis Bin Ali | Zhiyuan Tian |\n",
    "| Prakash K C | William Fung | Danny Jammooa | Mikayla Norton |\n",
    "| Santosh Chhetri | Max Chumley | Liantao Li  | Faith Houck  |\n",
    "| Sardar Nafis Bin Ali | Zhiyuan Tian | Steven Strachan  | Santosh Chhetri |\n",
    "| Shimeng Dai | Trent Henry | Fawaz Imtiaz | Angqi Li |\n",
    "| Shubham Koirala  | Eli Broemer | Md Faisal | Jeeva Saravana Bhavanandam |\n",
    "| Spencer Lee | Max Gregg | Arsh Ahtsham  | Ashkan Bagherzadeh Khodashahri |\n",
    "| Steven Strachan  | Santosh Chhetri | Max Chumley | Liantao Li  |\n",
    "| Trent Henry | Fawaz Imtiaz | Angqi Li | Spencer Lee |\n",
    "| William Fung | Danny Jammooa | Mikayla Norton | Hakan Burak Karli |\n",
    "| Yan Lyu | Neel Joshi  | Yuhan Zhu | Patrick Cook |\n",
    "| Yash Vikas Mandlecha | Maruf Md Ikram | Zhongren Xu | Josiah Hill  |\n",
    "| Yuhan Zhu | Patrick Cook | Patrick Govan | Bingqing Wang |\n",
    "| Zhiyuan Tian | Steven Strachan  | Santosh Chhetri | Max Chumley |\n",
    "| Zhongren Xu | Josiah Hill  | Kang Ho Lee  | Prakash K C |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TYi8kg2dCXxc"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
