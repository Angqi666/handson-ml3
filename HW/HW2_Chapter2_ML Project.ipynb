{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "579fcb38-2202-4bb3-970b-7815b43d9998",
   "metadata": {},
   "source": [
    "\n",
    "![TSC](https://media.licdn.com/dms/image/C4D12AQGQ7CjWKCQPSw/article-cover_image-shrink_600_2000/0/1583331061002?e=2147483647&v=beta&t=7SEdQ9ZC2l0FmEGTEAtNoCd1B7RcvFZoVIy0MIGl23c)\n",
    "\n",
    "\n",
    "\n",
    "<font size=8 color=#00FF11> Homework 2: Preparing for Machine Learning </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd2842b-a374-4a60-8113-70120c8f6d25",
   "metadata": {},
   "source": [
    "____\n",
    "\n",
    "Let's step back and think about what are learning this week in Chapter 2. We see that machine learning (ML) projects are quite involved! \n",
    "\n",
    "For today, let's break the process into five steps:\n",
    "1. data science (which was done in ICA 2)\n",
    "2. data preparation (<font color=#FF9900>which is what this HW covers</font>)\n",
    "3. ML (next week)\n",
    "4. metrics (next week)\n",
    "5. deploy\n",
    "\n",
    "Of course, not all ML projects exactly follow this plan. We'll adapt the spirit of this workflow as needed throughout the semester. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a811d3c9-8a3d-47c0-8457-4d82ad43f104",
   "metadata": {},
   "source": [
    "____\n",
    "<font color=#FFAA00> Read </font>\n",
    "\n",
    "![pen](https://findicons.com/files/icons/766/base_software/128/pencil3.png)\n",
    "\n",
    "Your first task is to completely read Chapter 2 of your textbook. Please send me any questions you have so that I can include them in the lecture next week. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee29eece-e834-4bf5-a37c-4da025820491",
   "metadata": {},
   "source": [
    "In this HW we will focus on the preparation steps. It would be useful for you to follow along starting on page 67 of your text. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a0eebc-0453-46ae-92c5-c4cac1ace8d2",
   "metadata": {},
   "source": [
    "____\n",
    "<font color=#FFAA00> Impute </font>\n",
    "\n",
    "![pen](https://findicons.com/files/icons/766/base_software/128/pencil3.png)\n",
    "\n",
    "Using the online documentation, research each of these methods and write a summary of what each does:\n",
    "* `dropna`\n",
    "* `fillna`\n",
    "* `drop`\n",
    "\n",
    "In your answer, include details: each of these methods has several options. \n",
    "\n",
    "Conclude your descriptions with your best guess on which of these might be preferred. Or, under what circumstances would you expect them to be the best choice? \n",
    "\n",
    "Apply each method to \"data\" in this code to help guide your answers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "520e24e9-ead0-48cb-a791-e6833e084c0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     A     B   C\n",
       "0  1.0   5.0  10\n",
       "1  2.0   NaN  20\n",
       "2  NaN   NaN  30\n",
       "3  4.0   8.0  40\n",
       "4  5.0  10.0  50"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Creating a simple dataset with missing values\n",
    "data = {\n",
    "    'A': [1, 2, np.nan, 4, 5],\n",
    "    'B': [5, np.nan, np.nan, 8, 10],\n",
    "    'C': [10, 20, 30, 40, 50]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer\n",
    "* Use `dropna` when you want to remove rows or columns with missing values.\n",
    "* Use `fillna` when you want to fill missing values with a specific value or method.\n",
    "* Use `drop` when you want to remove specific rows or columns.\n",
    "\n",
    "See the three cases I used for `dropna`, `fillna` and `drop`.\n",
    "* `dropna`: I use `df.dropna()` which mean is default parameters. which axis = 0 and it will drop all the row have 'NA'. Since the 'B' contain most 'N/A' I used subset to 'B' to remove all the 'NA' from 'B'. But since the data is too less which is not a good way to use `dropna` for this dataset. And in situations where the missing values are random, and removing them won't introduce bias into the dataset.\n",
    "\n",
    "* `fillna`: for the `fillna` I choose the value parameter set to median and which will fill the 'NA' with median calculate before(see in example code). That can propagate the value with some number which is based on other data. which will perserve most of the information in the data. But it will create some bias. When retaining the rows or columns with missing values is crucial. That will be the situation.\n",
    "\n",
    "* `drop`: 'B' column label and 'axis = 1' will be the location of which 'NA' will be drop in this case. label will be the index and axis = 1 is mean in the column-wise. Since the B column of the sample data contain the most 'N/A' so that drop the second column will be the best option to use `drop`. But it won't remove all the 'n/a'. In the case that use `drop` will be used is specific rows or columns are identified as outliers, irrelevant, or unnecessary for analysis.\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(     A     B   C\n",
       " 0  1.0   5.0  10\n",
       " 3  4.0   8.0  40\n",
       " 4  5.0  10.0  50,\n",
       "      A     B   C\n",
       " 0  1.0   5.0  10\n",
       " 1  2.0   8.0  20\n",
       " 2  8.0   8.0  30\n",
       " 3  4.0   8.0  40\n",
       " 4  5.0  10.0  50,\n",
       "      A   C\n",
       " 0  1.0  10\n",
       " 1  2.0  20\n",
       " 2  NaN  30\n",
       " 3  4.0  40\n",
       " 4  5.0  50)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new = df.copy()\n",
    "# Applying dropna to remove rows with NaN values\n",
    "df_dropped = df_new.dropna(subset=['B'])\n",
    "\n",
    "# Applying fillna to replace NaN values with 0\n",
    "median = df_new[\"B\"].median()\n",
    "df_filled = df_new.fillna(median)\n",
    "\n",
    "# Applying drop to remove the second row\n",
    "df_dropped_row = df_new.drop('B', axis=1)\n",
    "\n",
    "df_dropped,df_filled,df_dropped_row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "847fd09e-0c8a-4b71-8a13-c91348400900",
   "metadata": {},
   "source": [
    "_____\n",
    "\n",
    "As you can see, imputing data is tricky and we might need a more flexible tool. And, we'll need to have a tool that works with the complete ML workflow that includes breaking the data into training, validation and testing (review page 34). \n",
    "\n",
    "So far we have been using very simple tools from Pandas; let's upgrade to _scikit-learn_. \n",
    "\n",
    "____\n",
    "<font color=#FFAA00> Sklearn's API </font>\n",
    "\n",
    "\n",
    "![pen](https://findicons.com/files/icons/766/base_software/128/pencil3.png)\n",
    "\n",
    "Read page 70 of your textbook. I will cover this in the lecture, but it is really useful to understand the design of _scikit-learn_ so that you can use it very efficiently. \n",
    "\n",
    "In a markdown cell:\n",
    "* define \"API\"\n",
    "* describe what a \"transformer\" is in the context of _scikit-learn_. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer\n",
    " * **API**: Application Programming Interface, is bacially specifies how to call functions and serves as a contract between the library (e.g. scikit-learn) and the developers who use it. What parameters they accept, and what results they return. A well-designed API simplifies the usage of the library, provides a clear interface, and promotes good software design practices.\n",
    "\n",
    "* **Transformer**: is use to transform the data. After fit() we will have a “trained” imputer to transform the training set by replacing missing values with the learned data and use transform to replace the missing values. As is the case for a SimpleImputer use strategy median and fit() trained imputer have the median calculated and them transform(), which is replace the missing data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d288fc-b072-4fc6-a2de-8bf5994bd4e8",
   "metadata": {},
   "source": [
    "____\n",
    "<font color=#FFAA00> SimpleImputer </font>\n",
    "\n",
    "Let's turn to `SimpleImputer` in scikit-learn. \n",
    "\n",
    "![pen](https://findicons.com/files/icons/766/base_software/128/pencil3.png)\n",
    "\n",
    "Read the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html) carefully. In a markdown cell, summarize what \"strategy\" does, what the default is and how this compares to the other methods you summarized above. Conclude with your best assessment of how you would use this library. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "\n",
    "strategy parameter is use to define what methods (mean, median, most_frequent and constant) use to replace the missing data. (default=’mean’)\n",
    "* **mean and median** are typically used for numerical data.\n",
    "* **most_frequent** is suitable for imputing categorical data.\n",
    "* **constant** allows users to specify a constant value for imputation.\n",
    "\n",
    "If you mean compare to `drop`,`fillna` and `dropna`.  Both scikit-learn's SimpleImputer and pandas' drop, fillna, and dropna methods serve the purpose of handling missing values in a dataset. But SimpleImputer perserve more data information and better for building predictive models. The padas methods are for general data cleaning and manipulation tasks outside of machine learning workflows.\n",
    "\n",
    "SimpleImputer is fairly easy to deal with missing data with different strategy. The choice of strategy depends on the characteristics of the dataset. It's particularly useful in building machine learning pipelines for data preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ecc3d5-cc34-4d76-8fee-255ead00d95f",
   "metadata": {},
   "source": [
    "____\n",
    "<font color=#FFAA00> Code Practice </font>\n",
    "\n",
    "![pen](https://findicons.com/files/icons/766/base_software/128/pencil3.png)\n",
    "\n",
    "Next, let's walk through the code given in the documentation and understand the API. What does this code do? Specifically, what does it return? What kind of variable is `imp_mean` and what are we supposed to do with it? Why is there no data?!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c256570d-efff-487e-8005-423417b83bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2c0709-9170-48ec-af51-b88dd794c716",
   "metadata": {},
   "source": [
    "Let's make the fake data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07d48aaf-00dc-4805-b708-4ad556c5c3fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7.,  2.,  3.],\n",
       "       [ 4., nan,  6.],\n",
       "       [10.,  5.,  9.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_data = np.array([[7, 2, 3], [4, np.nan, 6], [10, 5, 9]])\n",
    "fake_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d87efb-d010-4ad8-9570-b7a7c24570e1",
   "metadata": {},
   "source": [
    "What does this do? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "    It make a 3 by 3 np.array with some missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2aca653b-3d25-46e1-8795-591889c6aa51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SimpleImputer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SimpleImputer</label><div class=\"sk-toggleable__content\"><pre>SimpleImputer()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SimpleImputer()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp_mean.fit(fake_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa28542d-2aed-40c1-be2c-6d45459c7a2c",
   "metadata": {},
   "source": [
    "Did anything happen? What do you see? What does `.fit()` do? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "<font color=#FFAA00>\n",
    "It didn't return any value only the SimpleImputer. The .fit() method is called on the imp_mean object with the fake_data as the argument. And it will train the imputer with exist data and calculate the column mean for next step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7062163-48f4-44f9-accc-53c1114888a3",
   "metadata": {},
   "source": [
    "What does transform do? Check its output: does it give what you expected? What if you change the strategy? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "<font color=#FFAA00>\n",
    "Replace missing values using the mean along each column. If we change to median it will replace the missing data with column median value. same for other strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fab5f8f5-640c-4273-97f0-1df3b9afc550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7. ,  2. ,  3. ],\n",
       "       [ 4. ,  3.5,  6. ],\n",
       "       [10. ,  5. ,  9. ]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp_mean.transform(fake_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0641b00-124e-44e6-80a4-9707ac2a65fc",
   "metadata": {},
   "source": [
    "If we combine these, what does `.fit_transform()` do? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7. ,  2. ,  3. ],\n",
       "       [ 4. ,  3.5,  6. ],\n",
       "       [10. ,  5. ,  9. ]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_data = np.array([[7, 2, 3], [4, np.nan, 6], [10, 5, 9]])\n",
    "imp_mean.fit_transform(fake_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "<font color=#FFAA00>\n",
    "It will directly give me the output after '.fit()' and '.transform()' "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e194e8-44f1-45d0-980b-6afb06ed088d",
   "metadata": {},
   "source": [
    "What if we apply the learned imputer to new data? Explain what this gives; is this what you expected? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "<font color=#FFAA00>\n",
    "the output still based on the old data that been trained on for the imputer and replaced the missing value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b61fc450-0bc1-4960-ad9c-807a3216f21c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7. , 2. , 6. ],\n",
       "       [4. , 3.5, 6. ],\n",
       "       [7. , 6. , 9. ]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp_mean.transform(np.array([[7, 2, np.nan], [4, np.nan, 6], [np.nan, 6, 9]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11749e93-3d93-4919-a0ba-ab1a617d6a82",
   "metadata": {},
   "source": [
    "Explain why we would want to use `.fit()` alone, and then `.transform()`, versus combining them in one step. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "\n",
    "* Use .fit() is when someone want to learn from the data without modifying it. and \n",
    "* Use .transform() when someone want to apply a learned transformation to new data.     \n",
    "* Use .fit_transform() when someone want to learn from and transform the training data in a single step.\n",
    "\n",
    "<font color=#FFAA00>\n",
    ".fit_transform() is convenient, but the separate steps can be beneficial in scenarios involving multiple datasets. like the case above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1de612-d52f-4bd3-96d5-8052d758439c",
   "metadata": {},
   "source": [
    "____\n",
    "<font color=#FFAA00> Encoding and Scaling </font>\n",
    "\n",
    "In the housing dataset we have many numerical values and most of the methods we have explored so far work fine. For example, the mean of a column of numbers is well defined. \n",
    "\n",
    "How do we deal with non-numeric value? In fact, what are non-numeric values? \n",
    "\n",
    "![pen](https://findicons.com/files/icons/766/base_software/128/pencil3.png)\n",
    "\n",
    "Answer all of these questions, each in its own markdown cell:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* what is an ordinal number?\n",
    "    \n",
    "    An ordinal number is a categorical variable where the categories have a meaningful order or ranking. Two nearby values are more similar than two distant values. For example in the textbook, for ordered categories such as “bad”, “average”, “good”, and “excellent”. \"good\" and \"excellent are nearby and they are more similar. But for the housing case the ocean_proximity doesn't follow that.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* what is a cardinal number?\n",
    "\n",
    "    The cardinal numbers are the numbers that are used for counting something. Such as \"three apples\" the 3 is a cardinal number. The basic cardinal numbers are typically the positive integers.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* how do ordinal and cardinal numbers differ fundamentally from an integer?\n",
    "\n",
    "    **Integers** are whole numbers that can be positive, negative, or zero. They are just values. \n",
    "    \n",
    "    **Cardinal numbers** represent the quantity or size of a set and answer the question \"how many?\" or \"how much?\" \n",
    "    \n",
    "    **Ordinal numbers** represent the order or position of an element in a sequence. Examples of ordinal numbers include 1st, 2nd, 3rd, 4th, and so on.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* what is a nominal number?\n",
    "\n",
    "    Nominal number is more like a tag and not to denote an actual value or quantity. like the number on the back of a player's basketball shirt. A number used to identify someone or something."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* in data science, what is \"encoding\"?\n",
    "\n",
    "    In data science, \"encoding\" refers to the process of converting categorical data or text-based data into a numerical format that can be used for machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* does scikit-learn contain libraries for ordinal encoding? is it the same as one-hot encoding? explain\n",
    "\n",
    "    Yes it have ordinal encoding. But it is different from one-hot encoding. But they serve for different categories.\n",
    "    \n",
    "    **Ordinal encoding** is used when there is a meaningful order among the categories, and preserving this order is important. like “bad”, “average”, “good”, and “excellent” case.\n",
    "    \n",
    "    **One-hot encoding** is used for nominal data which means the order are meaningless, and it represents each category as a binary vector. like\n",
    "    \n",
    "    ```\n",
    "    array([[0., 0., 0., 1., 0.],[1., 0., 0., 0., 0.], [0., 1., 0., 0., 0.], ...,\n",
    "    [0., 0., 0., 0., 1.], [1., 0., 0., 0., 0.], [0., 0., 0., 0., 1.]])\n",
    "    ```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* when and why would we use encoding as part of the data preparation within the ML pipeline?\n",
    "\n",
    "    Encoding is performed during the data preprocessing phase after imputing with strategy most frequent. The reason why we use encoding as part of the data preparation is that it handled categorical data make it learnable for the model and dealing with ordinal categorical features helps preserve the order of categories. Proper encoding contributes to the accuracy, effectiveness, and interpretability of the resulting models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* when would you use `MinMaxScalar` versus `StandardScalar`? what's the point of these libraries? \n",
    "\n",
    "    For `MinMaxScalar`, also called normalization is used when the features have different ranges. And you want to preserve the relationship because the models are tend to ignore the smaller range. For example in textbook case the tot_num_bedrooms is larger than median_income so that the model may ignore the median_income feature. It will be affected by the outliers.\n",
    "\n",
    "    For `StandardScalar`, also called standardization is used when you have outliers. First it subtracts the mean value (so standardized values have a zero mean). then it divides the result by the standard deviation (so standardized values have a standard deviation equal to 1). But for this one you can't not control the range of the data.\n",
    "\n",
    "    As I mentioned before models are more likely affect by the different range of the features. These libraries can  deal with datasets with features of varying magnitudes. So that all features can be considered by the models.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
