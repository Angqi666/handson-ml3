{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd966ac2-03a8-4633-8b84-cea7f3a1c0e3",
   "metadata": {},
   "source": [
    "____\n",
    "\n",
    "<font size=+3 color=#AA55FF> HW 5: Classification, Project and Optimization </font>\n",
    "\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83024c5a-c852-480d-ac35-73c895b8efb1",
   "metadata": {},
   "source": [
    "____\n",
    "\n",
    "<font size=+2 color=#FFAA00> Read (5 Points) </font>\n",
    "\n",
    "____\n",
    "\n",
    "Read chapter 4 of your text book and send me a message about anything that didn't make sense. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f41f5a-46aa-436b-964e-bec7c4ae0c8b",
   "metadata": {},
   "source": [
    "____\n",
    "\n",
    "<font size=+2 color=#FFAA00> Project Preproposal (35 Points) </font>\n",
    "\n",
    "____\n",
    "\n",
    "Last week you thought through your project and put together some initial ideas. This week you are going to write a preproposal. This will be reviewed by your ML colleagues in the class. \n",
    "\n",
    "The basic guidelines of your preproposal are:\n",
    "* professional format (see below),\n",
    "* minimum of 1200 words, (no maximum)\n",
    "* focus on the goal and possible datasets,\n",
    "* generate a PDF (submitted to D2L).\n",
    "\n",
    "Your goal is to convince us that you should attempt the project you have in mind. For this HW, your goal is not the details of the ML but selling the idea, which includes convincing us that:\n",
    "* this project is a good idea,\n",
    "* it will likely work.\n",
    "\n",
    "We are at the \"big picture\" stage right now.\n",
    "\n",
    "Your first step is to watch this video:\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7999ed02-f751-4863-8a35-40557eb008aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/LCwFmrXSnCs?si=0Fl4BjpQQNHxtmDc\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/LCwFmrXSnCs?si=0Fl4BjpQQNHxtmDc\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d55c05-2ceb-45c7-ac8d-413f323fbb05",
   "metadata": {},
   "source": [
    "Next, write a preproposal for your project. As I have mentioned many times, all projects are very different, so use all of this guidance in the \"spirit\" in which it is intended. For example, if you are building a robot and will train it using RL, your dataset is very different from a standard regression project. And, you will get feedback and will eventually write a _full proposal_, so don't worry about locking yourself into anything now. In fact, the process of first working through the preproposal is to help make your proposal much closer to what you will actually do. The proposal will require preliminary results. Here, use fake data or a `sklearn`/`seaborn`/UCI/Kaggle dataset if needed to show your plan. Use all of the steps you organized from the Delphi method in your narrative. \n",
    "\n",
    "\n",
    "## Deep Learning-based EEG Artifact Removal\n",
    "\n",
    "### Brief Description: \n",
    "\n",
    "This project aims to utilize deep learning techniques, specifically the Deep Image Prior approach, for EEG artifact removal, leveraging the dataset provided by the EEGdenoiseNet repository. The project will explore the feasibility of applying techniques from the ENCASE paper, which focuses on ECG classification using expert features and deep neural networks, to EEG data denoising. The ultimate goal is to develop an effective model for EEG artifact removal, contributing to advancements in EEG signal processing and improving the quality of EEG data for neuroscientific research.\n",
    "\n",
    "### Project Goals and Objectives\n",
    "\n",
    "* Ultimate Goal: Develop a deep learning model for EEG artifact removal using the Deep Image Prior approach.\n",
    "* Specific Objectives:\n",
    "    * Implement and evaluate deep learning techniques from the ENCASE paper on EEG artifact removal.\n",
    "    * Compare the performance of the proposed model with existing methods using the EEGdenoiseNet benchmark dataset.\n",
    "### Background and Motivation\n",
    "\n",
    "* Problem Statement: EEG signals often contain artifacts such as eye blinks, muscle activity, and environmental noise, which can distort the underlying neural activity and hinder accurate analysis. Removing these artifacts is crucial for obtaining clean EEG data for research and clinical applications.\n",
    "\n",
    "* Importance of the Problem: Clean EEG data is essential for various applications, including brain-computer interfaces, epilepsy diagnosis, and cognitive neuroscience research. Improving artifact removal techniques can enhance the reliability and interpretability of EEG data analysis.\n",
    "\n",
    "* Relevance to Machine Learning: Machine learning techniques, particularly deep learning, offer promising solutions for EEG artifact removal by leveraging large-scale datasets and complex feature representations.\n",
    "### Data Collection and Preparation\n",
    "\n",
    "* Data Sources: The dataset will be obtained from the EEGdenoiseNet repository, which provides clean EEG data and EEG data with eye artifacts.\n",
    "* Data Collection Methods: The dataset will be preprocessed and explored to understand its properties, including temporal dynamics and artifact characteristics.\n",
    "* Initial Data Analysis (IDA): Basic statistical analysis and visualization will be performed to assess the distribution and characteristics of the EEG data.\n",
    "* Exploratory Data Analysis (EDA):  Advanced analysis techniques, such as time-frequency analysis and spatial mapping, will be employed to uncover patterns and relationships in the EEG data.\n",
    "* Data Preparation: The data will be cleaned, transformed, and prepared for model training, including artifact removal and feature extraction. But the dataset have already clean and have the noisy segments included.\n",
    "\n",
    "### Methodology\n",
    "\n",
    "* Machine Learning Models: Deep learning models, including convolutional neural networks (CNNs) and recurrent neural networks (RNNs), will be explored for EEG artifact removal.\n",
    "* Feature Engineering: Expert features from the ENCASE paper will be adapted for EEG data denoising, alongside feature representations learned by deep neural networks.\n",
    "* Model Training: The models will be trained on the preprocessed EEG data using appropriate loss functions and optimization algorithms.\n",
    "* Model Evaluation and Tuning: The performance of the models will be evaluated using metrics such as signal-to-noise ratio (SNR) and classification accuracy. Hyperparameters will be tuned using cross-validation techniques.\n",
    "\n",
    "### Implementation Plan\n",
    "\n",
    "* Tools and Technologies: Python, TensorFlow, and PyTorch will be used for model implementation and training. Jupyter Notebooks will facilitate code development and experimentation.\n",
    "* Project Timeline: The project will be conducted over rest of semester, with milestones including data preprocessing, model development, evaluation, and documentation.\n",
    "\n",
    "### Metrics for Success\n",
    "\n",
    "* Evaluation Metrics: Metrics such as SNR improvement, artifact removal accuracy, and classification performance will be used to assess the effectiveness of the models.\n",
    "* Success Criteria: Success will be achieved by developing a model that outperforms existing methods on the EEGdenoiseNet benchmark dataset and demonstrates robustness across different artifact types and scenarios.\n",
    "\n",
    "### Communication of Results\n",
    "\n",
    "* Reporting: Progress updates and results will be documented in regular reports and presentations. A final report will summarize the project findings and outcomes.\n",
    "* Visualization: Visualizations, including EEG signal plots, model performance curves, and comparison charts, will be used to communicate findings effectively to stakeholders.\n",
    "\n",
    "### Risks and Challenges\n",
    "\n",
    "* Potential Risks: Identify any potential risks or obstacles that could hinder the project's success.\n",
    "* Mitigation Strategies: Propose strategies to mitigate these risks.\n",
    "\n",
    "### Ethical Considerations\n",
    "\n",
    "* Potential Risks: Challenges may include limited dataset size, model overfitting, and computational resource constraints.\n",
    "* Mitigation Strategies: Strategies such as data augmentation, regularization techniques, and cloud computing resources will be employed to address these risks.\n",
    "\n",
    "\n",
    "### References\n",
    "\n",
    "* Hong, Shenda, et al. \"ENCASE: An ENsemble ClASsifiEr for ECG classification using expert features and deep neural networks.\" 2017 Computing in cardiology (cinc). IEEE, 2017.\n",
    "* Hong, Shenda, et al. \"Combining deep neural networks and engineered features for cardiac arrhythmia detection from ECG recordings.\" Physiological measurement 40.5 (2019): 054009.\n",
    "* Jiang, Xiao, Gui-Bin Bian, and Zean Tian. \"Removal of artifacts from EEG signals: a review.\" Sensors 19.5 (2019): 987.\n",
    "* Zhang, Haoming, et al. \"EEGdenoiseNet: a benchmark dataset for deep learning solutions of EEG denoising.\" Journal of Neural Engineering 18.5 (2021): 056057."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2a9eb5-6e0b-4c35-b278-2be33277c97f",
   "metadata": {},
   "source": [
    "____\n",
    "\n",
    "<font size=+2 color=#FFAA00> Optimization (10 Points) </font>\n",
    "\n",
    "____\n",
    "\n",
    "Find the minima of these functions:\n",
    "\n",
    "\n",
    "\n",
    "In addition to giving the numerical values, comment on any issues that arose (if any did). What role did $\\lambda$ and $\\xi$ play? \n",
    "\n",
    "How would you visualize these functions? Plot each of them. ([This](https://jakevdp.github.io/PythonDataScienceHandbook/04.04-density-and-contour-plots.html) might give some useful hints, and [this style](https://plotly.com/python/3d-surface-plots/) is also useful/interactive.) If a parameter is not given, choose a few values. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364a1c80",
   "metadata": {},
   "source": [
    "---\n",
    "$$f(w,v) = (1 - (w + 2v))^2,$$\n",
    "Line of Minima: $$w + 2v = 1$$\n",
    "\n",
    "Do not have a minima point instead is a line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f295b053",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Define the function\n",
    "def f(w, v):\n",
    "    return (1 - (w + 2*v))**2\n",
    "\n",
    "# Generate values for w and v\n",
    "w_values = np.linspace(-10, 10, 100)\n",
    "v_values = np.linspace(-10, 10, 100)\n",
    "\n",
    "# Create a meshgrid for 3D plotting\n",
    "W, V = np.meshgrid(w_values, v_values)\n",
    "Z = f(W, V)\n",
    "\n",
    "# Plotting the function\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot_surface(W, V, Z, cmap='viridis')\n",
    "\n",
    "# Plot the line of minima (w + 2v = 1)\n",
    "w_min = np.linspace(-10, 10, 100)\n",
    "v_min = (1 - w_min) / 2\n",
    "ax.plot(w_min, v_min, f(w_min, v_min), color='red', label='Line of Minima')\n",
    "\n",
    "# Set labels and title\n",
    "ax.set_xlabel('w')\n",
    "ax.set_ylabel('v')\n",
    "ax.set_zlabel('f(w,v)')\n",
    "ax.set_title('Plot of f(w,v)')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02d2a8c",
   "metadata": {},
   "source": [
    "$$g(w,v) = (1 - (w + 2v))^2 + (\\pi - (w + 42v))^2,$$\n",
    "Line of Minima: $$8w+3544v−84π−4=0$$\n",
    "Do not have a minima point instead is a line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047876ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function\n",
    "def g(w, v):\n",
    "    return (1 - (w + 2*v))**2 + (np.pi - (w + 42*v))**2\n",
    "\n",
    "# Generate values for w and v\n",
    "w_values = np.linspace(-10, 10, 100)\n",
    "v_values = np.linspace(-10, 10, 100)\n",
    "\n",
    "# Create a meshgrid for 3D plotting\n",
    "W, V = np.meshgrid(w_values, v_values)\n",
    "Z = g(W, V)\n",
    "\n",
    "# Plotting the function\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot_surface(W, V, Z, cmap='viridis')\n",
    "\n",
    "# Plot the 8w+3544v−84π−4=0\n",
    "w_min = np.linspace(-10, 10, 100)\n",
    "v_min = (4+np.pi*84-8*w_min)/3544\n",
    "ax.plot(w_min, v_min, f(w_min, v_min),'r-', label='Minimum line')\n",
    "\n",
    "# Set labels and title\n",
    "ax.set_xlabel('w')\n",
    "ax.set_ylabel('v')\n",
    "ax.set_zlabel('g(w,v)')\n",
    "ax.set_title('Plot of g(w,v)')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f551e50f",
   "metadata": {},
   "source": [
    "---\n",
    "$$r(w,v) = \\lambda(w^2 + v^2),$$\n",
    "Minima point: $$w = 0, v = 0$$\n",
    "\n",
    "`note:` $\\lambda$  `adjust the z-axis value, but shape of the graph is not change. (is a scaler)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff27310c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Define the function\n",
    "def r(w, v, lam):\n",
    "    return lam * (w**2 + v**2)\n",
    "\n",
    "# Generate values for w and v\n",
    "w_values = np.linspace(-10, 10, 100)\n",
    "v_values = np.linspace(-10, 10, 100)\n",
    "\n",
    "# Create a meshgrid for 3D plotting\n",
    "W, V = np.meshgrid(w_values, v_values)\n",
    "\n",
    "# Set lambda\n",
    "lam = 20 # lambda can be adjust\n",
    "\n",
    "# Calculate the function values\n",
    "R = r(W, V, lam)\n",
    "\n",
    "# Plotting the function\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot_surface(W, V, R, cmap='viridis')\n",
    "\n",
    "# Plot the minimum point\n",
    "ax.scatter(0, 0, r(0, 0, lam), color='red', label='Minimum')\n",
    "\n",
    "# Set labels and title\n",
    "ax.set_xlabel('w')\n",
    "ax.set_ylabel('v')\n",
    "ax.set_zlabel('r(w,v)')\n",
    "ax.set_title('Plot of r(w,v)')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e35680",
   "metadata": {},
   "source": [
    "---\n",
    "$$s(w,v) = \\xi (|w| + |v|),$$\n",
    "Minima point: $$w = 0, v = 0$$\n",
    "\n",
    "`note:` $\\xi$  `adjust the z-axis value, but shape of the graph is not change. (is a scaler)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b163fffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Define the function\n",
    "def s(w, v, xi):\n",
    "    return xi * (np.abs(w) + np.abs(v))\n",
    "\n",
    "# Generate values for w and v\n",
    "w_values = np.linspace(-10, 10, 100)\n",
    "v_values = np.linspace(-10, 10, 100)\n",
    "W, V = np.meshgrid(w_values, v_values)\n",
    "\n",
    "# Set xi\n",
    "xi = 1  # You can adjust the value of xi as needed\n",
    "\n",
    "# Calculate the function values\n",
    "S = s(W, V, xi)\n",
    "\n",
    "# Plotting the function\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot_surface(W, V, S, cmap='viridis')\n",
    "\n",
    "# Plot the minimum point\n",
    "ax.scatter(0, 0, s(0, 0, xi), color='red', label='Minimum (w=0, v=0)')\n",
    "\n",
    "# Set labels and title\n",
    "ax.set_xlabel('w')\n",
    "ax.set_ylabel('v')\n",
    "ax.set_zlabel('s(w,v)')\n",
    "ax.set_title('Plot of s(w,v)')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b99fb8",
   "metadata": {},
   "source": [
    "---\n",
    "$$r + f,$$\n",
    "when $\\lambda = 1$\n",
    "$$(w,v)=(\\dfrac{1}{6},\\dfrac{1}{3})$$\n",
    "As increase the $\\lambda$ the minima will approach $$ w = 0, v = 0$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b7a761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate values for w and v\n",
    "w_values = np.linspace(-10, 10, 100)\n",
    "v_values = np.linspace(-10, 10, 100)\n",
    "W, V = np.meshgrid(w_values, v_values)\n",
    "\n",
    "# Set values of lambda\n",
    "lambda_values = [1, 10000]  # You can adjust lambda values as needed\n",
    "\n",
    "# Create subplots for each lambda value\n",
    "fig, axs = plt.subplots(1, len(lambda_values), figsize=(15, 5))\n",
    "\n",
    "\n",
    "for i, lam in enumerate(lambda_values):\n",
    "    # Calculate the function values for the current lambda\n",
    "    R = r(W, V, lam) + f(W, V)\n",
    "    \n",
    "    # Plotting the function\n",
    "    ax = axs[i]\n",
    "    ax = fig.add_subplot(1, len(lambda_values), i+1, projection='3d')\n",
    "    ax.plot_surface(W, V, R, cmap='viridis')\n",
    "    ax.set_xlabel('w')\n",
    "    ax.set_ylabel('v')\n",
    "    ax.set_zlabel('r(w,v)')\n",
    "    ax.set_title(f'Plot of r(w,v) with lambda={lam}')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe94bc27",
   "metadata": {},
   "source": [
    "---\n",
    "$$r + s$$\n",
    " Minima point:\n",
    "$$(w,v)=(0,0)$$\n",
    "$\\lambda$ and $\\xi$ are the weight of each function. increasing one of each will indeed affect the dominance of the respective functions in the combined function $$ r(w,v)+s(w,v).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5925b685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate values for w and v\n",
    "w_values = np.linspace(-10, 10, 100)\n",
    "v_values = np.linspace(-10, 10, 100)\n",
    "W, V = np.meshgrid(w_values, v_values)\n",
    "\n",
    "# Set values of lambda\n",
    "lambda_values = [1, 100]  # You can adjust lambda values as needed\n",
    "xi_values = [1,100]\n",
    "\n",
    "\n",
    "# Create subplots for each combination of lambda and xi\n",
    "fig, axs = plt.subplots(len(lambda_values), len(xi_values), figsize=(15, 10))\n",
    "\n",
    "for i, lam in enumerate(lambda_values):\n",
    "    for j, xi in enumerate(xi_values):\n",
    "        # Calculate the function values for the current lambda and xi\n",
    "        R = r(W, V, lam) + s(W, V, xi)\n",
    "        # Calculate the subplot index\n",
    "        subplot_index = i * len(xi_values) + j + 1\n",
    "        # Plotting the function\n",
    "        ax = axs[i]\n",
    "        ax = fig.add_subplot(2, 2, subplot_index, projection='3d')\n",
    "        ax.plot_surface(W, V, R, cmap='viridis')\n",
    "        ax.set_xlabel('w')\n",
    "        ax.set_ylabel('v')\n",
    "        ax.set_zlabel('r(w,v) + s(w,v)')\n",
    "        ax.set_title(f'Plot of r(w,v) with lambda={lam} and s(w,v) with xi={xi}')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5eead5-7983-421c-a498-cbbec7600ece",
   "metadata": {},
   "source": [
    "____\n",
    "\n",
    "<font size=+2 color=#FFAA00> Multilabel Classification (10 Points) </font>\n",
    "\n",
    "____\n",
    "\n",
    "Most of the applications of classification we have looked at have a single output; in this problem you will explore multilabel classification. You have a lot of experience, so you are mostly on your own. Review the section of your textbook on multilabel classification if you need to (p. 125). \n",
    "\n",
    "To start:\n",
    "* write a code to read in the penguins dataset from Seaborn,\n",
    "* do standard IDA, EDA,\n",
    "* clean as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ac73c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load penguins dataset from Seaborn\n",
    "penguins = sns.load_dataset('penguins')\n",
    "\n",
    "# Initial Data Analysis (IDA)\n",
    "# Display basic information about the dataset\n",
    "print(\"Basic Information about the Penguins Dataset:\")\n",
    "print(penguins.info())\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(\"\\nFirst few rows of the Penguins Dataset:\")\n",
    "print(penguins.head())\n",
    "\n",
    "# Exploratory Data Analysis (EDA)\n",
    "# Visualize the distribution of the target variable\n",
    "sns.countplot(x='species', data=penguins)\n",
    "plt.title('Distribution of Penguin Species')\n",
    "plt.show()\n",
    "\n",
    "# Visualize the relationships between variables\n",
    "sns.pairplot(penguins, hue='species')\n",
    "plt.suptitle('Pairplot of Penguin Features', y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing Values in the Penguins Dataset:\")\n",
    "print(penguins.isnull().sum())\n",
    "\n",
    "# Clean the dataset if needed\n",
    "# For simplicity, let's drop rows with missing values\n",
    "penguins_cleaned = penguins.dropna()\n",
    "\n",
    "# Verify that the dataset is cleaned\n",
    "print(\"\\nCleaned Penguins Dataset:\")\n",
    "print(penguins_cleaned.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c70bdd",
   "metadata": {},
   "source": [
    "---\n",
    "Next, we need to make a multilabel dataset. As you saw from the IDA and EDA there are four real inputs and three categorical features. Next, follow these steps:\n",
    "* break/separate the real features into an input matrix $X$,\n",
    "* break/separate the categorical features into a $y$ matrix (note this is a matrix this time, not a vector!!),\n",
    "* encode $y$ (e.g., using one-hot encoding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9aab7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary library\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Separate real features into input matrix X\n",
    "X = penguins_cleaned[['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']]\n",
    "\n",
    "# Separate categorical features into y matrix\n",
    "y_categorical = penguins_cleaned[['species', 'island', 'sex']]\n",
    "\n",
    "# Initialize the OneHotEncoder\n",
    "encoder = OneHotEncoder() \n",
    "\n",
    "# Fit and transform the categorical features\n",
    "y_encoded = encoder.fit_transform(y_categorical).toarray()\n",
    "\n",
    "# Combine X and y_encoded to create the final multilabel dataset\n",
    "multilabel_dataset = pd.concat([X.reset_index(drop=True), pd.DataFrame(y_encoded, columns=encoder.get_feature_names_out(['species', 'island', 'sex']))], axis=1)\n",
    "\n",
    "# Display the final multilabel dataset\n",
    "print(\"\\nFinal Multilabel Dataset:\")\n",
    "print(multilabel_dataset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e6202c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Now you have a multilabel dataset. Perform the next obvious steps:\n",
    "* train-test split,\n",
    "* scaling,\n",
    "* etc....you know the drill by now....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd4ddbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Assuming 'multilabel_dataset' is your combined dataset\n",
    "\n",
    "# Separate features (X) and labels (y) from the multilabel dataset\n",
    "X = multilabel_dataset.drop(columns=['species_Adelie', 'species_Chinstrap', 'species_Gentoo', 'island_Biscoe', 'island_Dream', 'island_Torgersen', 'sex_Female', 'sex_Male'])\n",
    "y = multilabel_dataset[['species_Adelie', 'species_Chinstrap', 'species_Gentoo', 'island_Biscoe', 'island_Dream', 'island_Torgersen', 'sex_Female', 'sex_Male']]\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scaling (using StandardScaler)\n",
    "\n",
    "knn_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('knn', KNeighborsClassifier()),\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ccbd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {'k': range(1,3), 's': [0.5, 0.7, 1.0]}\n",
    "score = 'f1_macro'\n",
    "\n",
    "clf = GridSearchCV(MLkNN(), parameters, scoring=score)\n",
    "clf.fit(X, y)\n",
    "\n",
    "print (clf.best_params_, clf.best_score_)\n",
    "\n",
    "# output\n",
    "({'k': 1, 's': 0.5}, 0.78988303374297597)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67499664",
   "metadata": {},
   "source": [
    "Use [_K-Nearest Neighbors_](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) for the classifier. Your code _might_ look like:\n",
    "\n",
    "`knn_pipeline.fit(X_train, y_train)`\n",
    "\n",
    "`y_pred = knn_pipeline.predict(X_test)`\n",
    "\n",
    "`accuracy = accuracy_score(y_test, y_pred)`\n",
    "\n",
    "Vary $K$. \n",
    "\n",
    "Make a classification report and add a discussion of what you learn:\n",
    "\n",
    "`print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, target_names=encoder.get_feature_names_out()))`.\n",
    "\n",
    "Finally, research using the docs to understand what the metrics and scores mean for the case of multilabel. Which ones make the most sense? Write a detailed answer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef5c99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report,ConfusionMatrixDisplay,confusion_matrix\n",
    "\n",
    "# Train the model\n",
    "knn_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = knn_pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, target_names=encoder.get_feature_names_out()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4083ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'knn__n_neighbors': np.arange(1, 20)}\n",
    "\n",
    "# Create GridSearchCV\n",
    "grid_search = GridSearchCV(knn_pipeline, param_grid, cv=3, scoring='f1_micro')\n",
    "\n",
    "# Fit the model to the data\n",
    "knn_1 = grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and corresponding accuracy\n",
    "print(\"Best Parameters: \", grid_search.best_params_)\n",
    "print(\"Best Cross-Validated Accuracy: {:.2f}\".format(grid_search.best_score_))\n",
    "\n",
    "y_pred_1 = knn_1.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred_1)\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_1, target_names=encoder.get_feature_names_out()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3d21b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
