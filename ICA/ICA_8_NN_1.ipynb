{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7386303d-9278-411e-a9bc-0a935ec488db",
   "metadata": {},
   "source": [
    "![AA](https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Autoencoders-graph.png?resize=768%2C576&ssl=1)\n",
    "____\n",
    "\n",
    "<font size=+3 color=#AA55FF> ICA: Tensorflow and Autoencoders </font>\n",
    "\n",
    "____\n",
    "\n",
    "It's almost spring break!\n",
    "\n",
    "In this ICA we have two goals:\n",
    "* learn a bit about the `Tensorflow` (TF) library\n",
    "* learn how to do dimensionality reduction with an autoencoder.\n",
    "\n",
    "All of the great libraries in `sklearn` are always there for you to use: there is no reason to not use `sklearn` and `Tensorflow` together. As `Tensorflow` is more concerned with deep learning, `sklearn` offers data preprocessing and other ML estimators not in `Tensorflow`. \n",
    "\n",
    "As you work through this ICA, think about whether your project could use deep learning or high performance computing (e.g., use of [GPUs](https://www.tensorflow.org/guide/gpu)). \n",
    "\n",
    "We will use a familiar dataset today, the MNIST handwritten digits, so that you don't need to learn a new dataset and the visualizations are easy to interpret. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95153acc-e99e-4a67-8b37-2168d86a389f",
   "metadata": {},
   "source": [
    "____\n",
    "\n",
    "<font size=+1 color=#AA55FF> Structure: Classification and Dimensionality Reduction </font>\n",
    "\n",
    "____\n",
    "\n",
    "First, I am going to give you some TF code that does classification of the MNIST data. This will familiarize you with the steps TF uses and what some of the options are. Then, you will code the autoencoder (AE) portion, with less guidance.\n",
    "\n",
    "The first thing you need to do is ensure everyone in your group has TF installed. Take a moment to ensure that is true, and test it by running the next code cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736fd2de-c3c1-4517-93fe-f3162117edc7",
   "metadata": {},
   "source": [
    "____\n",
    "\n",
    "<font size=+1 color=#FF55AA> Classification  </font>\n",
    "\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a3269fc-3d16-4a90-8be1-1f8e9d074dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Reshape\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191d881a-0911-4669-86db-ab4d61752cf5",
   "metadata": {},
   "source": [
    "____\n",
    "\n",
    "<font color=#FF55AA> Data Preparation  </font>\n",
    "____\n",
    "\n",
    "Run the code below and comment on what it does. For example, what datatypes are being used? How do you get one input image and display it? What is the `to_categorical` doing here? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31ff55db-276e-4ab1-8904-a1ab63819093",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load MNIST data\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# scale the images to [0,1]\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "# one-hot encoding\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244f2324-12a6-42e2-af72-b59b2709ad02",
   "metadata": {},
   "source": [
    "____\n",
    "\n",
    "<font color=#FF55AA> Build TF Model  </font>\n",
    "____\n",
    "\n",
    "Examine this syntax to understand the logic/API of how you specify a NN to TF. Describe in your own words what each step is doing: `Sequential`, `Flatten`, etc. Use the [online docs](https://www.tensorflow.org/guide/keras/sequential_model) to see other ways to do the same thing, such as using `.add`. What other activation functions are available? Why is softmax used here? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8408170-d1bc-4d0a-aa76-8a665bd24e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-21 13:12:48.206669: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# simple feedforward neural network model\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),  # input layer: Flatten the 28x28 images\n",
    "    Dense(128, activation='relu'),  # hidden layer with 128 neurons and ReLU activation\n",
    "    Dense(10, activation='softmax')  # output layer with 10 neurons (for 10 digits) and softmax activation\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ae447d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "631db12d-a713-4f09-ad4f-183519efa97c",
   "metadata": {},
   "source": [
    "____\n",
    "\n",
    "<font color=#FF55AA> Training the NN  </font>\n",
    "____\n",
    "\n",
    "With the NN architecture specified, we can train it. Discuss the code below with your group and describe what it does. Vary the parameters to see how they impact the accuracy. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4deeef-671d-4a75-b6c6-49b3e7beec31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# train/fit the model\n",
    "model.fit(train_images, train_labels, epochs=5, batch_size=32)\n",
    "\n",
    "# evaluate the model on test data\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "\n",
    "print(f'Test accuracy: {test_acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fed0a1-2c72-49d4-b8f1-d02760e38cb9",
   "metadata": {},
   "source": [
    "____\n",
    "\n",
    "<font color=#FF55AA> Classification Summary  </font>\n",
    "____\n",
    "\n",
    "You have started your career in deep learning - congrats! ü•≥\n",
    "\n",
    "As you can see, the TF API is very well designed. In fact, the true TF API is not so easy because TF is a very low-level library (think of programming in machine language). It is the `Keras` wrapper that makes TF so easy for us to use, which is why you see that name in the `import` statements. \n",
    "\n",
    "If you plan to do classification in your project, especially if your dataset is very large, consider using TF. You can, and should, compare with many other estimators, so you can also include several estimators from `sklearn`, if that makes sense for you. You have some starter code üëÜüèª up there and there are many places you can take it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea09441-242b-438e-8217-a36f6c7365e2",
   "metadata": {},
   "source": [
    "____\n",
    "\n",
    "<font size=+1 color=#FF55AA> Dimensionality Reduction  </font>\n",
    "____\n",
    "\n",
    "The classification problem you just completed is fairly standard in two ways: (1) there are many ways to do classification and that NN is just one particular variant, and (2) the NN itself is a feed forward (sequential) architecture, perhaps the simplest one.\n",
    "\n",
    "To fully grasp the wide applicability of NNs in general, we will use TF to build a NN that does something completely different: dimensionality reduction in an unsupervised setting. \n",
    "\n",
    "The architecture we will use is called an <font color=#0055FF>autoencoder</font> (AE), which attempts to produce the same information on its input and output; that is, it \"does nothing\". \n",
    "\n",
    "The key idea is the low-dimensional latent space, which is what you will vary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bb4399-f913-4a14-a229-7955877ce90c",
   "metadata": {},
   "source": [
    "____\n",
    "\n",
    "<font color=#FF55AA> Build AE  </font>\n",
    "____\n",
    "\n",
    "Let's start with some preprocessing. Look at the figure at the top of this notebook; this is roughly what we want to build. To make the connection to the figure clearer, we will flatten the data first. Confirm that this works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d2cc51-8f3e-445f-80ea-47492849acfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten the images\n",
    "train_images = train_images.reshape((len(train_images), -1))\n",
    "test_images = test_images.reshape((len(test_images), -1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130dfb6b-2e02-43f0-8b0e-e7e2ed180359",
   "metadata": {},
   "source": [
    "Next, we need to specific how many dimensions to go down to, and we'll want to vary this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f5b15b-129d-4f70-9924-f2b3a9e34822",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_dim = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc2288c-d179-4719-94b9-1004697eddf7",
   "metadata": {},
   "source": [
    "We will use the [`Model` library](https://keras.io/api/models/model/) (read the docs in this link) in TF to build the AE, so that you can see how this works. As you will see, this allows you to build very diverse NNs. \n",
    "\n",
    "Examine this code, discuss it and write your comments on how it works. Note that it adds new layers in a slightly different way: it creates a layer and passes that as the input to a function that creates the next layer, then repeats until done. You might want to draw a picture of what each step is doing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb0202a-bd9b-493d-8b2c-a4947a2fa45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder model\n",
    "encoder_input = tf.keras.Input(shape=(784,))\n",
    "encoded = Dense(128, activation='relu')(encoder_input)\n",
    "encoded = Dense(encoding_dim, activation='relu')(encoded)\n",
    "\n",
    "# decoder model\n",
    "decoded = Dense(128, activation='relu')(encoded)\n",
    "decoded_output = Dense(784, activation='sigmoid')(decoded)\n",
    "\n",
    "# AE\n",
    "autoencoder = Model(encoder_input, decoded_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7345069-29ac-4c90-a9ba-90e8f529fad0",
   "metadata": {},
   "source": [
    "The reason for doing it this way is that you will often want to use just the encoder or decoder parts separately after training. Remember, the overall goal of the AE is to \"do nothing\"! If we didn't want access to the latent space and just wanted to train an AE, we could use this code. Don't uncomment it so that it doesn't interfere with the other code - I just want you to see an alternate way to use the TF API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2b7496-57e6-45c4-b38f-5e990766c628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_img = Input(shape=(784,))\n",
    "# encoded = Dense(128, activation='relu')(input_img)\n",
    "# encoded = Dense(32, activation='relu')(encoded)  # 32-dimensional encoded representation\n",
    "# decoded = Dense(128, activation='relu')(encoded)\n",
    "# decoded = Dense(784, activation='sigmoid')(decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2dcdff1-38f2-4c08-80da-b8c4c4e6950e",
   "metadata": {},
   "source": [
    "Ok, let's build _just_ the encoder and _just_ the decoder; we'll use this to probe the AE and make images below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c5ad03-716b-4c22-82dc-c10e787972b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# just encoder: only to the ltent space\n",
    "encoder = Model(encoder_input, encoded)\n",
    "\n",
    "# just decoder: starts from the latent space\n",
    "encoded_input = tf.keras.Input(shape=(encoding_dim,))\n",
    "decoder_layer1 = autoencoder.layers[-2]\n",
    "decoder_layer2 = autoencoder.layers[-1]\n",
    "decoder = Model(encoded_input, decoder_layer2(decoder_layer1(encoded_input)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1e484c-bc30-4061-a235-0e167fd240e1",
   "metadata": {},
   "source": [
    "____\n",
    "\n",
    "<font color=#FF55AA> Train AE  </font>\n",
    "____\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26edd545-8e34-4e9b-8364-8d6add3a852f",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "# Train the autoencoder\n",
    "autoencoder.fit(train_images, train_images,\n",
    "                epochs=5,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(test_images, test_images))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e0b167-c3e3-401b-a7cf-65ca60b7b108",
   "metadata": {},
   "source": [
    "____\n",
    "\n",
    "<font color=#FF55AA> Confusion!!  </font>\n",
    "____\n",
    "\n",
    "The code below allows you to visualize what the AE is doing. Here is the logic of the code:\n",
    "* two images are selected\n",
    "* these two images are placed in the low-dimensionality latent space\n",
    "* the images are _interpolated in the latent space_, creating a continuous new image there\n",
    "* that new image is then decoded to see what the AE thinks it is! (color images are originals)\n",
    "* play with the code, be sure you understand it, and be sure to vary the dimensions in the latent space! can you confuse the AE by going to small??\n",
    "\n",
    "Note that if the dimensionality is too low, you cannot quite recover the original images. Run many times, and for many latent spaces, epochs and batch sizes. Summarize what you learned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899249b1-6e1c-4008-b6c8-97fc80bca5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_interpolations(encoder, decoder, data, n=10, figsize=(20, 4)):\n",
    "    \"\"\"\n",
    "    Visualizes the interpolation between two images in the latent space.\n",
    "    \n",
    "    Parameters:\n",
    "    - encoder: The encoder model.\n",
    "    - decoder: The decoder model.\n",
    "    - data: The dataset to use for sampling images.\n",
    "    - n: Number of interpolation steps.\n",
    "    - figsize: Size of the figure.\n",
    "    \"\"\"\n",
    "    \n",
    "    # select two random images from the dataset\n",
    "    idx = np.random.choice(len(data), 2, replace=False)\n",
    "    img1, img2 = data[idx]\n",
    "    \n",
    "    # encode the images to get their latent representations\n",
    "    z1, z2 = encoder.predict(np.array([img1, img2]))\n",
    "    \n",
    "    # interpolate between the latent representations\n",
    "    interpolations = np.zeros((n, z1.shape[-1]))\n",
    "    for i in range(n):\n",
    "        interpolations[i] = z1 + (z2 - z1) * i / (n - 1)\n",
    "    \n",
    "    # decode the interpolated representations\n",
    "    reconstructions = decoder.predict(interpolations)\n",
    "\n",
    "    # plotting\n",
    "    plt.figure(figsize=figsize)\n",
    "    for i, img in enumerate(reconstructions):\n",
    "        ax = plt.subplot(2, n, i + 1)\n",
    "        plt.imshow(img.reshape(28, 28), cmap='gray')\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "    \n",
    "    # plot the original images\n",
    "    for i, img in enumerate([img1, img2]):\n",
    "        ax = plt.subplot(2, n, 11 + i*9)\n",
    "        plt.imshow(img.reshape(28, 28), cmap='plasma')\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "plot_interpolations(encoder, decoder, test_images)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:homl3]",
   "language": "python",
   "name": "conda-env-homl3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
